{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment analysis on movie review.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMXRtgXoOGbYb8mATyF7W34",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhushalMitbaokar/DataScience/blob/main/sentiment_analysis_on_movie_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12-fx_vSfYWj",
        "outputId": "ccc394de-ec2d-4960-ed2b-472be3836773"
      },
      "source": [
        "!pip install sagemaker==1.72.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sagemaker==1.72.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/75ea837e2bd704b1567bdf55f7e768745da4fcf1e3b3e061e41ba7d7f399/sagemaker-1.72.0.tar.gz (297kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 8.0MB/s \n",
            "\u001b[?25hCollecting boto3>=1.14.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/3d/386cc84db1e57aa7782eed00bcbdb884e496bdb1689c7f4c09a22572846d/boto3-1.17.35-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from sagemaker==1.72.0) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.1 in /usr/local/lib/python3.7/dist-packages (from sagemaker==1.72.0) (3.12.4)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from sagemaker==1.72.0) (1.4.1)\n",
            "Collecting protobuf3-to-dict>=0.1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/55/522bb43539fed463275ee803d79851faaebe86d17e7e3dbc89870d0322b9/protobuf3-to-dict-0.1.5.tar.gz\n",
            "Collecting smdebug-rulesconfig==0.1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/2c/d7/80252c50e8848101914457d1cf58ef7e20f34406fc660d26108a1fec866d/smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from sagemaker==1.72.0) (3.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from sagemaker==1.72.0) (20.9)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.6MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/1d/e6ea5f2f856262415eb252b035fdb3524eeff4bb27864f7363a9bce5439f/botocore-1.20.35-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 20.4MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.1->sagemaker==1.72.0) (54.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.35->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c6/d3e3abe5b4f4f16cf0dfc9240ab7ce10c2baa0e268989a4e3ec19e90c84e/urllib3-1.26.4-py2.py3-none-any.whl (153kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 55.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: sagemaker, protobuf3-to-dict\n",
            "  Building wheel for sagemaker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sagemaker: filename=sagemaker-1.72.0-py2.py3-none-any.whl size=386358 sha256=2e175d753ff5c06ef9144ea07d90673e9b5b1893669a6a234c905a24934d1860\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/66/8b/47e7b3de18248022283c8a0207493ec5da81405f0a0a39426f\n",
            "  Building wheel for protobuf3-to-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-cp37-none-any.whl size=4030 sha256=3827e4752464655f9528d5d94723f5c4a21d6d873b079a3414581728fad00617\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/42/d8/1609d310cabebc2cf60eca038fa2b0c8503412963734a6fc31\n",
            "Successfully built sagemaker protobuf3-to-dict\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, protobuf3-to-dict, smdebug-rulesconfig, sagemaker\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.17.35 botocore-1.20.35 jmespath-0.10.0 protobuf3-to-dict-0.1.5 s3transfer-0.3.6 sagemaker-1.72.0 smdebug-rulesconfig-0.1.4 urllib3-1.26.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3zwe1zpEOKi",
        "outputId": "dd382815-4f77-44cd-d4f4-d6e7393ae37b"
      },
      "source": [
        "%mkdir ../data\n",
        "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-24 16:54:24--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
            "\n",
            "../data/aclImdb_v1. 100%[===================>]  80.23M  44.2MB/s    in 1.8s    \n",
            "\n",
            "2021-03-24 16:54:26 (44.2 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_u_9SG4ETcD"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def read_imdb_data(data_dir='../data/aclImdb'):\n",
        "    data = {}\n",
        "    labels = {}\n",
        "    \n",
        "    for data_type in ['train', 'test']:\n",
        "        data[data_type] = {}\n",
        "        labels[data_type] = {}\n",
        "        \n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            data[data_type][sentiment] = []\n",
        "            labels[data_type][sentiment] = []\n",
        "            \n",
        "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
        "            files = glob.glob(path)\n",
        "            \n",
        "            for f in files:\n",
        "                with open(f) as review:\n",
        "                    data[data_type][sentiment].append(review.read())\n",
        "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
        "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
        "                    \n",
        "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
        "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
        "                \n",
        "    return data, labels\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCed5rP-EeGE",
        "outputId": "688522b8-6979-421d-ce70-ac8f95a476d3"
      },
      "source": [
        "data, labels = read_imdb_data()\n",
        "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
        "            len(data['train']['pos']), len(data['train']['neg']),\n",
        "            len(data['test']['pos']), len(data['test']['neg'])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5t5wjvoEVH8"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def prepare_imdb_data(data, labels):\n",
        "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
        "    \n",
        "    #Combine positive and negative reviews and labels\n",
        "    data_train = data['train']['pos'] + data['train']['neg']\n",
        "    data_test = data['test']['pos'] + data['test']['neg']\n",
        "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
        "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
        "    \n",
        "    #Shuffle reviews and corresponding labels within training and test sets\n",
        "    data_train, labels_train = shuffle(data_train, labels_train)\n",
        "    data_test, labels_test = shuffle(data_test, labels_test)\n",
        "    \n",
        "    # Return a unified training data, test data, training labels, test labets\n",
        "    return data_train, data_test, labels_train, labels_test\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEZawlA6EjjJ",
        "outputId": "5aaddcb6-7c0a-41ab-9f2b-c8cfe746f5b2"
      },
      "source": [
        "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
        "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMDb reviews (combined): train = 25000, test = 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hggHs8frEpwe",
        "outputId": "277a0a2a-527b-49e2-ac17-a766c482bc51"
      },
      "source": [
        "print(train_X[100])\n",
        "print(train_y[100])\n",
        "print(test_X[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On the back burner for years (so it was reported) this television reunion of two of the most beloved characters in sitcom history started off badly - and went straight downhill from there. Mary Richards (Mary Tyler Moore) and her best friend Rhoda Morgenstern (Valerie Harper) meet in New York after a long estrangement and catch up on each other's lives. What a novel concept! But, sad to relate, nothing worth talking about (let alone making a movie about) has happened to either of them in the intervening years. So, instead, the script contents itself with throwing out one hoary old plot device after another (most having to do with older women in the workplace), while completely missing the quirky charm and sophistication that made the original show a winner. The supporting cast is instantly forgettable, the humor is nonexistent, and the chemistry which Moore and Harper once had together is gone. Moore allegedly stalled this project for years, waiting for \"just the right script\" before committing herself. If this was the one she considered \"right\", what on earth were the ones she turned down like? It's not the age of the characters that does this in (for time inevitably marches on), but the almost complete lack of imagination coupled with a blatant disregard for the elements that made the series work. At one time this was intended as a pilot but, all to obviously, it failed to generate any interest among potential sponsors. Or for that matter, among potential audiences. Quickly and mercifully forgotten, the film is a travesty and an insult to a classic.\n",
            "0\n",
            "Looking back on Jim Henson's works years after his death is like taking a look back into another time. For unlike most so-called creative types attempting to sell to, or worse yet cynically exploit, children nowadays, Jim never seemed to really forget what it was like to be a child. And if there ever was a moment in which he demonstrated this, Labyrinth aside, it is 1979's Muppet Movie. Filmed as an allegorical story about how Henson came to work in children's television as a puppeteer and ended up with a half-hour show of his own on primetime television, The Muppet Movie ends up an affirmation of everything more progressive, understanding sorts would say to children who did not quite meet the expected norm during the 1980s. And as we enjoy the fruits of an era in which we are gagged and bound from speaking about anything lest someone might get offended, the open celebration of difference or diversity that formed a large part of The Muppet Show is on offer here. I have said it in other comments, but I must say it here. A great light in the world went out the day Jim Henson died.<br /><br />The Muppet Movie begins with its cast sitting down to see the premiere of what were about to see for the next eighty or so minutes. In short, precise strokes, we are introduced to the major players as well as some of the minors. And when the story proper begins, boy are we given a great song to bring us into the moment. The Rainbow Connection painted both a beautiful and sad image of what the Muppets, especially Kermit, were. These were not just a bunch of felt puppets with singular personalities who combined to put on a show. They were living things based upon a part of all of us, only writ so much more boldly than we are used to. As each Muppet was introduced to us in turn, we saw another reflection of part of ourselves, and of course the children in the audience would respond differently to each character. Hence, everyone had a favourite. When Animal appeared behind a drum kit and attempted to eat a cymbal, I knew I had found one of mine. Nowadays, I am more of a Swedish Chef fan, but what the hey.<br /><br />Complementing the characters was a string of musical numbers that further developed their motivations and personalities. Can You Picture That? shared an insight into Dr. Teeth and his band as well as the creative soul of Henson. But the most relevant song to me was Gonzo's number, asking what he is and where he came from. Many of us would spend a lifetime gazing into the stars and, like Gonzo, saying we knew we would be going back there someday. Not that all the songs were so deadly serious, of course. Fozzy and Kermit share a number after they decide to combine their talent (or lack thereof) and hit the road. If any evidence were required that present-day \"musicians\" have lost the ability to use the pop structure to create something listenable, this number would be it. Never before, and never again, would the group dynamic of a cast and the music so perfectly complement one another. With the puppeteers and voice actors so perfectly on top of their game, the human cast had a lot to live up to.<br /><br />Which makes it all the more amazing that the human element also lived up to their end of the deal. Cameos literally pepper the film, with everyone from Steve Martin to Telly Savalas popping in to offer their support. Even Richard Pryor, the last man one would expect to see in a film about the Muppets, appears to set up a hilarious moment. Mel Brooks' cameo is just as disturbing to me as an adult as it was when I was a small boy, but I suspect that is because Henson knew why I would find it disturbing now. The big acting strength, though, comes from Charles Durning, who as Doc Hopper embodies everything both Henson and his audience determined to resist. At every junction, Hopper comes to either offer Kermit the chance to sell out and betray his own kind. Or perhaps offer stops being the right word when Hopper's attempts to ensure Kermit's compliance become progressively more forceful and violent. The whole thing is one big metaphor for how every artist has his heart broken by the world.<br /><br />Of course, Animal also shows up to remind us that just because our friends are not sweet and cuddly does not make them any less of a friend. In point of fact, Animal turns out to be the best friend that Kermit has in that moment. And that has been the core message of every good show or film Henson has been involved in ever since. That shunning or dismissing others simply because of linguistic or cosmetic differences could literally be the worst mistake one ever makes. There can be little doubt that in today's world where a moron in a purple suit can tell my sons they are not good if they do not have good feelings for fifteen seasons and still not come under serious investigation by child welfare authorities, Henson's creature workshop could never have got off the ground. To misquote the album title, daring to be stupid is one thing, but enforcing the choice upon others is another matter. The Muppet Movie demonstrates how Henson dared to ask us all to think, both inside and outside of the proverbial box. There will never be another entirely like him, but he never would want us to stop trying.<br /><br />Therefore, The Muppet Movie is the epitome of a ten out of ten film. If we were to send a film out into the cosmos to prove to intelligent life that we are worth more than being obliterated, this would be it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS3uzkbRE5Le"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def review_to_words(review):\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "    stemmer = PorterStemmer()\n",
        "    \n",
        "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
        "    words = text.split() # Split string into words\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
        "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
        "    \n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKEW4HhlISBE",
        "outputId": "9c11f24b-f0a7-43a8-f013-a5e59028973a"
      },
      "source": [
        "print(review_to_words(train_X[100]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['back', 'burner', 'year', 'report', 'televis', 'reunion', 'two', 'belov', 'charact', 'sitcom', 'histori', 'start', 'badli', 'went', 'straight', 'downhil', 'mari', 'richard', 'mari', 'tyler', 'moor', 'best', 'friend', 'rhoda', 'morgenstern', 'valeri', 'harper', 'meet', 'new', 'york', 'long', 'estrang', 'catch', 'live', 'novel', 'concept', 'sad', 'relat', 'noth', 'worth', 'talk', 'let', 'alon', 'make', 'movi', 'happen', 'either', 'interven', 'year', 'instead', 'script', 'content', 'throw', 'one', 'hoari', 'old', 'plot', 'devic', 'anoth', 'older', 'women', 'workplac', 'complet', 'miss', 'quirki', 'charm', 'sophist', 'made', 'origin', 'show', 'winner', 'support', 'cast', 'instantli', 'forgett', 'humor', 'nonexist', 'chemistri', 'moor', 'harper', 'togeth', 'gone', 'moor', 'allegedli', 'stall', 'project', 'year', 'wait', 'right', 'script', 'commit', 'one', 'consid', 'right', 'earth', 'one', 'turn', 'like', 'age', 'charact', 'time', 'inevit', 'march', 'almost', 'complet', 'lack', 'imagin', 'coupl', 'blatant', 'disregard', 'element', 'made', 'seri', 'work', 'one', 'time', 'intend', 'pilot', 'obvious', 'fail', 'gener', 'interest', 'among', 'potenti', 'sponsor', 'matter', 'among', 'potenti', 'audienc', 'quickli', 'merci', 'forgotten', 'film', 'travesti', 'insult', 'classic']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-4Jm_4CIUs4"
      },
      "source": [
        "\n",
        "import pickle\n",
        "\n",
        "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
        "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
        "\n",
        "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
        "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
        "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
        "\n",
        "    # If cache_file is not None, try to read from it first\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = pickle.load(f)\n",
        "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass  # unable to read from cache, but that's okay\n",
        "    \n",
        "    # If cache is missing, then do the heavy lifting\n",
        "    if cache_data is None:\n",
        "        # Preprocess training and test data to obtain words for each review\n",
        "        #words_train = list(map(review_to_words, data_train))\n",
        "        #words_test = list(map(review_to_words, data_test))\n",
        "        words_train = [review_to_words(review) for review in data_train]\n",
        "        words_test = [review_to_words(review) for review in data_test]\n",
        "        \n",
        "        # Write to cache file for future runs\n",
        "        if cache_file is not None:\n",
        "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
        "                              labels_train=labels_train, labels_test=labels_test)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
        "    else:\n",
        "        # Unpack data loaded from cache file\n",
        "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
        "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
        "    \n",
        "    return words_train, words_test, labels_train, labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "UQhuu0XgIcmX",
        "outputId": "e16dfaf0-2c63-466e-9731-a61c688858b8"
      },
      "source": [
        "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrote preprocessed data to cache file: preprocessed_data.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRki9ivbIfwp"
      },
      "source": [
        "test_X[10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iHa3x3fIySF"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_dict(data, vocab_size = 5000):\n",
        "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
        "    \n",
        "    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
        "    #       sentence is a list of words.\n",
        "    \n",
        "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
        "\n",
        "    for data_item in data:\n",
        "        for word in data_item:\n",
        "            if word not in word_count.keys():\n",
        "                word_count[word] = 1\n",
        "            else:\n",
        "                word_count[word] = word_count[word] +1\n",
        "                \n",
        "    \n",
        "    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
        "    #       sorted_words[-1] is the least frequently appearing word.\n",
        "    \n",
        "    i=0\n",
        "    sorted_words = []\n",
        "    weight = []\n",
        "    for key, value in word_count.items():\n",
        "        sorted_words.append(key)\n",
        "        weight.append(value)\n",
        "    \n",
        "    i = 0\n",
        "    for i in range(len(sorted_words)):\n",
        "        j = 0\n",
        "        for j in range(len(sorted_words)):\n",
        "            if weight[j] <= weight[i]:\n",
        "                temp1 = sorted_words[i]\n",
        "                sorted_words[i] = sorted_words[j]\n",
        "                sorted_words[j] = temp1\n",
        "                temp2 = weight[i]\n",
        "                weight[i] = weight[j]\n",
        "                weight[j] = temp2\n",
        "    print(sorted_words)\n",
        "    print(weight)\n",
        "    \n",
        "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
        "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
        "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
        "        \n",
        "    return word_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhhINtjbIzzo"
      },
      "source": [
        "word_dict = build_dict(train_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZHbY1wtI2bb"
      },
      "source": [
        "print(list(word_dict.items())[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RUYGj2ULsem"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmo9Ir9LJLOZ",
        "outputId": "bcf6d80e-8df9-42ec-eded-3469034f3ad3"
      },
      "source": [
        "import pandas as pd  \n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "train = pd.read_csv('labeledTrainData.tsv', header=0, delimiter='\\t', quoting=3)     \n",
        "\n",
        "def review_to_words(raw_review):\n",
        "    removedTags = BeautifulSoup(raw_review)         #1. Remove HTML\n",
        "    upperAndLowerRemains = re.sub('[^a-zA-Z]',\" \",removedTags.get_text()) #2. Remove non letters\n",
        "    toLowerAndSplit = upperAndLowerRemains.lower().split() #3. Convert to lowercase and split it into words\n",
        "    stops = set(stopwords.words('english'))\n",
        "    stopwordsRemoved = [w for w in toLowerAndSplit if not w in stops]  #4. Remove stops words\n",
        "    complete_review = \" \".join(stopwordsRemoved);  #5. Joint back and return the joined sentence\n",
        "    return complete_review\n",
        "    \n",
        "CleanedListOfReviews = []\n",
        "BagOfWords = []\n",
        "for iterator in range(0,train[\"review\"].size): \n",
        "    if iterator%1000 == 0 or iterator==24999:     #Checking progress after every 1000 Reviews\n",
        "        print(\"Cleaned Reviews: \",iterator)\n",
        "    complete_review = review_to_words(train[\"review\"][iterator])\n",
        "    CleanedListOfReviews.append(complete_review)\n",
        "    BagOfWords.append(train[\"sentiment\"][iterator])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vocabularySize = 5000\n",
        "smoothingFactor = 5\n",
        "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = vocabularySize)  # Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \n",
        "\n",
        "data_features = vectorizer.fit_transform(CleanedListOfReviews)\n",
        "data_features = data_features.toarray()\n",
        "\n",
        "trainingSet = 0.8\n",
        "dataSize = train['review'].size\n",
        "trainSize = dataSize * trainingSet\n",
        "SentimentsTrainedSet = []\n",
        "ReviewsTrainedSet = []\n",
        "ReviewsValidationSet = []\n",
        "SentimentsValidationSet = []\n",
        "for i in range( 0, dataSize):\n",
        "\tif(i < trainSize):\n",
        "\t\tSentimentsTrainedSet.append(BagOfWords[i])\n",
        "\t\tReviewsTrainedSet.append(data_features[i])\n",
        "\telse:\n",
        "\t\tSentimentsValidationSet.append(BagOfWords[i])\n",
        "\t\tReviewsValidationSet.append(data_features[i])\n",
        "\n",
        "\n",
        "\n",
        "# Fitting the model to Naive Bayes Classifier\n",
        "clf = MultinomialNB(alpha=smoothingFactor)\n",
        "clf.fit(np.array(ReviewsTrainedSet), np.array(SentimentsTrainedSet))\n",
        "\n",
        "\n",
        "#Predicting on Validation set\n",
        "pred_labels = clf.predict(np.array(ReviewsValidationSet))\n",
        "val_labels = np.array(SentimentsValidationSet)\n",
        "\n",
        "#Calculating Accuracy\n",
        "accuracy = float((pred_labels == val_labels).sum())\n",
        "total = val_labels.size\n",
        "\n",
        "acc_perc = (accuracy/total)*100\n",
        "print(\"\\nAccuracy on 20% validation set with smoothing factor \",smoothingFactor,\" and vocabulary size \",vocabularySize,\" is: \",acc_perc)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Cleaned Reviews:  0\n",
            "Cleaned Reviews:  1000\n",
            "Cleaned Reviews:  2000\n",
            "Cleaned Reviews:  3000\n",
            "Cleaned Reviews:  4000\n",
            "Cleaned Reviews:  5000\n",
            "Cleaned Reviews:  6000\n",
            "\n",
            "Accuracy on 20% validation set with smoothing factor  5  and vocabulary size  5000  is:  86.321094312455\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}